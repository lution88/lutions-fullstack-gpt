{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(KR)**\n",
    "\n",
    "- 앞서 배운 메모리 클래스 중 하나를 사용하는 메모리로 LCEL 체인을 구현합니다.\n",
    "- 이 체인은 영화 제목을 가져와 영화를 나타내는 세 개의 이모티콘으로 응답해야 합니다. (예: \"탑건\" -> \"🛩️👨‍✈️🔥\". \"대부\" -> \"👨‍👨‍👦🔫🍝\").\n",
    "- 항상 세 개의 이모티콘으로 답장하도록 `FewShotPromptTemplate` 또는 `FewShotChatMessagePromptTemplate`을 사용하여 체인에 예시를 제공하세요.\n",
    "- 메모리가 작동하는지 확인하려면 체인에 두 개의 영화에 대해 질문한 다음 다른 셀에서 체인에 먼저 질문한 영화가 무엇인지 알려달라고 요청하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content=['1']), AIMessage(content=['1']), HumanMessage(content=['2']), AIMessage(content=['2']), HumanMessage(content=['3']), AIMessage(content=['3']), HumanMessage(content=['4']), AIMessage(content=['4'])]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content=['3']), AIMessage(content=['3']), HumanMessage(content=['4']), AIMessage(content=['4']), HumanMessage(content=['5']), AIMessage(content=['5']), HumanMessage(content=['6']), AIMessage(content=['6'])]}\n"
     ]
    }
   ],
   "source": [
    "add_message(5, 5)\n",
    "add_message(6, 6)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제부터 LLM 사용\n",
    "\n",
    "summary - 요약해줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"bupyung is so pretty\", \"i wisg i could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it's cool. Lution mentions how pretty Bupyung is, and the AI expresses a wish to go there.\"}\n"
     ]
    }
   ],
   "source": [
    "print(get_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it's cool. Lution mentions how pretty Bupyung is, and the AI expresses a wish to go there.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 summary + buffer\n",
    "\n",
    "오래된 애들은 요약함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return print(memory.load_memory_variables({}))\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")\n",
    "add_message(\"i like a meat\", \"wow that is so cool!\")\n",
    "add_message(\"I like a IU who is famous singer.\", \"wow she is so great singer!\")\n",
    "add_message(\"i do crossfit workout everyday.\", \"wow you are so nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"i like playing piano.\", \"wow nice work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it\\'s cool that Lution lives there. The human mentions liking IU, a famous singer, and the AI agrees, saying she is a great singer. The human shares that they do crossfit workouts everyday, to which the AI responds with admiration, calling them nice. The human then mentions liking playing piano, to which the AI responds with a compliment, saying \"wow nice work!\"'}\n"
     ]
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 중요한 요약본 뽑는거\n",
    "\n",
    "대화 중 엔티티의 knowledge graph 만들기\n",
    "\n",
    "knowledget graph에서 히스토리를 가지고 오지 않고 엔티티를 가져오기 때문에 get_history 삭제\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On lution: lution lives in Bupyung.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is lution?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"lution likes movies of marvle's\", \"wow that is so awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"On lution: lution lives in Bupyung. lution likes movies of marvle's.\")]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"what does lution likes?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL 에서 memory 사용하는법\n",
    "- memory를 chain 에 꽂는 방법\n",
    "\n",
    "- 두 종류의 chain을 사용해서 꽂는 방법\n",
    "\n",
    "LLM Chain: off-the-shelf chain -> 일반적인 체인\n",
    "\n",
    "커스텀 Chain: LCEL 을 활용해서 만들수있음.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mmy name is nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf chain 사용기 - LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 예는 predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in bupyung.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Bupyung is a district in Incheon, South Korea. What do you enjoy most about living in Bupyung?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory 을 사용한 LLM 체인은 이전 데이터를 저장하지 못함.\n",
    "\n",
    "디버깅 해보자 - verbose=True 사용\n",
    "\n",
    "LLMChain 은 이전에 한 말을 기억하지 못하지만 memory 는 기억한다.\n",
    "\n",
    "그렇다면 문제는 memory 가 프롬프트에 포함되지 않는다는 것.\n",
    "\n",
    "해결방법은 프롬프트에 포함시키자\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human: my name is nico\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf chain 사용기 - LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=120, memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# template 추가\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 예는 predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: Nico introduces themselves to the AI, who greets Nico and offers assistance. Nico mentions they live in Bupyung, and the AI acknowledges this and offers help with information or services related to Bupyung. The human asks the AI for their name, and the AI responds that their name is Nico. The human mentions they live in Bupyung, and the AI offers assistance with information or services related to Bupyung.\n",
      "Human: I live in bupyung.\n",
      "AI: It seems like you mentioned that you live in Bupyung multiple times. Is there something specific you would like to know or discuss about Bupyung?\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know, Nico! How can I assist you with information or services related to Bupyung?\n",
      "    Human: my name is nico\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: Nico introduces themselves to the AI, who greets Nico and offers assistance. Nico mentions they live in Bupyung, and the AI acknowledges this and offers help with information or services related to Bupyung. The human asks the AI for their name, and the AI responds that their name is Nico. The human mentions they live in Bupyung, and the AI offers assistance with information or services related to Bupyung. The human reiterates that they live in Bupyung, prompting the AI to inquire if there is something specific they would like to know or discuss about Bupyung.\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know, Nico! How can I assist you with information or services related to Bupyung?\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "    Human: I live in bupyung.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Nico! How can I assist you with information or services related to Bupyung? Is there something specific you would like to know or discuss about Bupyung?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: my name is nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know! How can I assist you with information or services related to Bupyung?\n",
      "    Human: what is my name?\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: my name is nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know! How can I assist you with information or services related to Bupyung?\n",
      "Human: what is my name?\n",
      "AI: Your name is Nico.\n",
      "    Human: I live in bupyung.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! How can I assist you with information or services related to Bupyung?\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대화형 Memory 사용\n",
    "- ChatPromptTemplate 사용\n",
    "- MessagesPlaceholder : 대화형 메세지를 담을 공간\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human.\n",
      "Human: my name is nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful AI talking to a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 예는 predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL 로 커스텀 chain 에 memory 추가하기\n",
    "- chain.invoke 사용\n",
    "- RunnablePassthrough.assign() 사용 \n",
    "  : 프롬프트가 format 되기 전에 우리가 함수를 실행시키는 걸 허락해줌.\n",
    "   그리고 이 함수에서 우리가 원하는 어떤 값이든 변수에 할당 가능.\n",
    "   그 변수는 prompt 로 전달됨.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful AI talking a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Lution! How can I assist you today?')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.invoke() - case 1\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"],\n",
    "        \"question\": \"My name is lution\",\n",
    "    }\n",
    ")\n",
    "# 이 접근방식의 문제는 체인을 호출할 때마다 chat_history도 추가해 주어야 한다는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    \"\"\"RunnablePassthrough.assing 은 사용자 input을 받는다.\"\"\"\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Lution! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"my name is lution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Lution.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
