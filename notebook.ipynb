{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(KR)**\n",
    "\n",
    "- ì•ì„œ ë°°ìš´ ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ë¡œ LCEL ì²´ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "- ì´ ì²´ì¸ì€ ì˜í™” ì œëª©ì„ ê°€ì ¸ì™€ ì˜í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„¸ ê°œì˜ ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: \"íƒ‘ê±´\" -> \"ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\". \"ëŒ€ë¶€\" -> \"ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\").\n",
    "- í•­ìƒ ì„¸ ê°œì˜ ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ë‹µì¥í•˜ë„ë¡Â `FewShotPromptTemplate`Â ë˜ëŠ”Â `FewShotChatMessagePromptTemplate`ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì— ì˜ˆì‹œë¥¼ ì œê³µí•˜ì„¸ìš”.\n",
    "- ë©”ëª¨ë¦¬ê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ ì²´ì¸ì— ë‘ ê°œì˜ ì˜í™”ì— ëŒ€í•´ ì§ˆë¬¸í•œ ë‹¤ìŒ ë‹¤ë¥¸ ì…€ì—ì„œ ì²´ì¸ì— ë¨¼ì € ì§ˆë¬¸í•œ ì˜í™”ê°€ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ë‹¬ë¼ê³  ìš”ì²­í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello'), HumanMessage(content='hi'), AIMessage(content='hello')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content=['1']), AIMessage(content=['1']), HumanMessage(content=['2']), AIMessage(content=['2']), HumanMessage(content=['3']), AIMessage(content=['3']), HumanMessage(content=['4']), AIMessage(content=['4'])]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content=['3']), AIMessage(content=['3']), HumanMessage(content=['4']), AIMessage(content=['4']), HumanMessage(content=['5']), AIMessage(content=['5']), HumanMessage(content=['6']), AIMessage(content=['6'])]}\n"
     ]
    }
   ],
   "source": [
    "add_message(5, 5)\n",
    "add_message(6, 6)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œë¶€í„° LLM ì‚¬ìš©\n",
    "\n",
    "summary - ìš”ì•½í•´ì¤Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"bupyung is so pretty\", \"i wisg i could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it's cool. Lution mentions how pretty Bupyung is, and the AI expresses a wish to go there.\"}\n"
     ]
    }
   ],
   "source": [
    "print(get_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it's cool. Lution mentions how pretty Bupyung is, and the AI expresses a wish to go there.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—” summary + buffer\n",
    "\n",
    "ì˜¤ë˜ëœ ì• ë“¤ì€ ìš”ì•½í•¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return print(memory.load_memory_variables({}))\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")\n",
    "add_message(\"i like a meat\", \"wow that is so cool!\")\n",
    "add_message(\"I like a IU who is famous singer.\", \"wow she is so great singer!\")\n",
    "add_message(\"i do crossfit workout everyday.\", \"wow you are so nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"i like playing piano.\", \"wow nice work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: Lution introduces themselves as living in Bupyung, and the AI responds enthusiastically, saying it\\'s cool that Lution lives there. The human mentions liking IU, a famous singer, and the AI agrees, saying she is a great singer. The human shares that they do crossfit workouts everyday, to which the AI responds with admiration, calling them nice. The human then mentions liking playing piano, to which the AI responds with a compliment, saying \"wow nice work!\"'}\n"
     ]
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°€ì¥ ì¤‘ìš”í•œ ìš”ì•½ë³¸ ë½‘ëŠ”ê±°\n",
    "\n",
    "ëŒ€í™” ì¤‘ ì—”í‹°í‹°ì˜ knowledge graph ë§Œë“¤ê¸°\n",
    "\n",
    "knowledget graphì—ì„œ íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì§€ê³  ì˜¤ì§€ ì•Šê³  ì—”í‹°í‹°ë¥¼ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— get_history ì‚­ì œ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": {input}}, {\"output\": {output}})\n",
    "\n",
    "\n",
    "add_message(\"hi, i'm lution. i live in bupyung\", \"wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On lution: lution lives in Bupyung.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is lution?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"lution likes movies of marvle's\", \"wow that is so awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"On lution: lution lives in Bupyung. lution likes movies of marvle's.\")]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"what does lution likes?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL ì—ì„œ memory ì‚¬ìš©í•˜ëŠ”ë²•\n",
    "- memoryë¥¼ chain ì— ê½‚ëŠ” ë°©ë²•\n",
    "\n",
    "- ë‘ ì¢…ë¥˜ì˜ chainì„ ì‚¬ìš©í•´ì„œ ê½‚ëŠ” ë°©ë²•\n",
    "\n",
    "LLM Chain: off-the-shelf chain -> ì¼ë°˜ì ì¸ ì²´ì¸\n",
    "\n",
    "ì»¤ìŠ¤í…€ Chain: LCEL ì„ í™œìš©í•´ì„œ ë§Œë“¤ìˆ˜ìˆìŒ.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mmy name is nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf chain ì‚¬ìš©ê¸° - LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ì˜ˆëŠ” predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in bupyung.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Bupyung is a district in Incheon, South Korea. What do you enjoy most about living in Bupyung?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory ì„ ì‚¬ìš©í•œ LLM ì²´ì¸ì€ ì´ì „ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ëª»í•¨.\n",
    "\n",
    "ë””ë²„ê¹… í•´ë³´ì - verbose=True ì‚¬ìš©\n",
    "\n",
    "LLMChain ì€ ì´ì „ì— í•œ ë§ì„ ê¸°ì–µí•˜ì§€ ëª»í•˜ì§€ë§Œ memory ëŠ” ê¸°ì–µí•œë‹¤.\n",
    "\n",
    "ê·¸ë ‡ë‹¤ë©´ ë¬¸ì œëŠ” memory ê°€ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ.\n",
    "\n",
    "í•´ê²°ë°©ë²•ì€ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚¤ì\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human: my name is nico\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off-the-shelf chain ì‚¬ìš©ê¸° - LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=120, memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# template ì¶”ê°€\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ì˜ˆëŠ” predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: Nico introduces themselves to the AI, who greets Nico and offers assistance. Nico mentions they live in Bupyung, and the AI acknowledges this and offers help with information or services related to Bupyung. The human asks the AI for their name, and the AI responds that their name is Nico. The human mentions they live in Bupyung, and the AI offers assistance with information or services related to Bupyung.\n",
      "Human: I live in bupyung.\n",
      "AI: It seems like you mentioned that you live in Bupyung multiple times. Is there something specific you would like to know or discuss about Bupyung?\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know, Nico! How can I assist you with information or services related to Bupyung?\n",
      "    Human: my name is nico\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: Nico introduces themselves to the AI, who greets Nico and offers assistance. Nico mentions they live in Bupyung, and the AI acknowledges this and offers help with information or services related to Bupyung. The human asks the AI for their name, and the AI responds that their name is Nico. The human mentions they live in Bupyung, and the AI offers assistance with information or services related to Bupyung. The human reiterates that they live in Bupyung, prompting the AI to inquire if there is something specific they would like to know or discuss about Bupyung.\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know, Nico! How can I assist you with information or services related to Bupyung?\n",
      "Human: my name is nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "    Human: I live in bupyung.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Nico! How can I assist you with information or services related to Bupyung? Is there something specific you would like to know or discuss about Bupyung?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: my name is nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know! How can I assist you with information or services related to Bupyung?\n",
      "    Human: what is my name?\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: my name is nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in bupyung.\n",
      "AI: That's great to know! How can I assist you with information or services related to Bupyung?\n",
      "Human: what is my name?\n",
      "AI: Your name is Nico.\n",
      "    Human: I live in bupyung.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! How can I assist you with information or services related to Bupyung?\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in bupyung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëŒ€í™”í˜• Memory ì‚¬ìš©\n",
    "- ChatPromptTemplate ì‚¬ìš©\n",
    "- MessagesPlaceholder : ëŒ€í™”í˜• ë©”ì„¸ì§€ë¥¼ ë‹´ì„ ê³µê°„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human.\n",
      "Human: my name is nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful AI talking to a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# ì˜ˆëŠ” predict? -\n",
    "chain.predict(question=\"my name is nico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL ë¡œ ì»¤ìŠ¤í…€ chain ì— memory ì¶”ê°€í•˜ê¸°\n",
    "- chain.invoke ì‚¬ìš©\n",
    "- RunnablePassthrough.assign() ì‚¬ìš© \n",
    "  : í”„ë¡¬í”„íŠ¸ê°€ format ë˜ê¸° ì „ì— ìš°ë¦¬ê°€ í•¨ìˆ˜ë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” ê±¸ í—ˆë½í•´ì¤Œ.\n",
    "   ê·¸ë¦¬ê³  ì´ í•¨ìˆ˜ì—ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì–´ë–¤ ê°’ì´ë“  ë³€ìˆ˜ì— í• ë‹¹ ê°€ëŠ¥.\n",
    "   ê·¸ ë³€ìˆ˜ëŠ” prompt ë¡œ ì „ë‹¬ë¨.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful AI talking a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Lution! How can I assist you today?')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.invoke() - case 1\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"],\n",
    "        \"question\": \"My name is lution\",\n",
    "    }\n",
    ")\n",
    "# ì´ ì ‘ê·¼ë°©ì‹ì˜ ë¬¸ì œëŠ” ì²´ì¸ì„ í˜¸ì¶œí•  ë•Œë§ˆë‹¤ chat_historyë„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def load_memory(input):\n",
    "    \"\"\"RunnablePassthrough.assing ì€ ì‚¬ìš©ì inputì„ ë°›ëŠ”ë‹¤.\"\"\"\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Lution! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"my name is lution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Lution.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
